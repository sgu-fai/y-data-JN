{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7jn3RRSSGbb"
   },
   "source": [
    "# Probabilistic classifier of texts into spam / ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1XGIBQPBSGbi"
   },
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GArFA9f2SGbk"
   },
   "source": [
    "Here is a classical \"complete the notebook\" assignment. \n",
    "\n",
    "You can run all the cells in the notebook, and some of them you have to complete. \n",
    "\n",
    "The code you have to complete is marked with `#TODO` comments. The cells containing such code also contain assertions that you should fulfill. \n",
    "\n",
    "If the cells produce no errors, you can be pretty sure you do everything OK. \n",
    "\n",
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UoWcpibiSGbu"
   },
   "outputs": [],
   "source": [
    "def square_root(x):\n",
    "    \"\"\" This is a function that takes a non-negative numeric argument x and produces its square root. \"\"\"\n",
    "    # TODO: calculate the square root of x and put it into the y variable instead of None. \n",
    "    # If you are not sure, have a look on the list of Python basic operators\n",
    "    # https://www.tutorialspoint.com/python/python_basic_operators.htm\n",
    "    y = x**0.5\n",
    "    return y\n",
    "\n",
    "assert square_root(144) == 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhdue65hSGb8"
   },
   "source": [
    "Now that you understand the format, let's have look at a [Naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) of short messages into spam and not-spam.\n",
    "\n",
    "The main idea behind it is that $$P(spam|text) = \\frac{P(spam)P(text|spam)}{P(text)}$$\n",
    "\n",
    "You will have to implement this formula along with some hacks to make its application more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OA3FpI3BSGb_"
   },
   "source": [
    "![](https://pics.me.me/suppose-you-have-one-rabbit-now-suppose-someone-gives-you-21826742.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Bb-0QvpSGcA"
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbW707TKUlqq"
   },
   "source": [
    "The cell below loads the file with messages. \n",
    "\n",
    "If you run this notebook locally on Windows, you have to download the file manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "AmmPwJzlSRm9",
    "outputId": "d6d4a060-c431-4302-d05d-e8580eb31add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: wget: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/avidale/ps4ds2019/master/homework/week1/spam_classifier/SMSSpamCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PX3xYoFQSGcC"
   },
   "source": [
    "The following cell imports some Python libraries. It is possible that you have some of them not installed (namely, `pandas`). In this case, you have to install them using package manager from command line. The command would look like `pip install pandas` or `conda install pandas`.\n",
    "\n",
    "If you run this notebook from Google Colab, then the libraries are already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tld2ItvwSGcD"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9ecc6a884b43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load some useful Python libratries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;31m# the library for working with data tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m \u001b[0;31m# a class for counting objects (words and text labels, in our case)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# load some useful Python libratries\n",
    "\n",
    "import pandas as pd # the library for working with data tables\n",
    "import re\n",
    "from collections import Counter # a class for counting objects (words and text labels, in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_yol8UpISGcG"
   },
   "outputs": [],
   "source": [
    "# load the data from disk to a tabular format, and give readable names to its columns\n",
    "data = pd.read_csv('SMSSpamCollection', sep='\\t', header=None)\n",
    "data.columns = ['target', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dAaMxDESGcP"
   },
   "source": [
    "In this dataset, \"ham\" is a good text, and \"spam\" is, well, spam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "OPRYIT4uSGcQ",
    "outputId": "dc0dadd1-3b63-417d-abf1-aa21997f8aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                                                                                                                                         text\n",
       "0    ham                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
       "1    ham                                                                                                                                Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
       "3    ham                                                                                                            U dun say so early hor... U c already then say...\n",
       "4    ham                                                                                                Nah I don't think he goes to usf, he lives around here though"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enable pandas to display large texts and look into our data\n",
    "\n",
    "pd.options.display.max_colwidth = 300\n",
    "\n",
    "print(data.shape) # number of rows and columns\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UkoK7Y5SGcW"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4RJ3Q_BBSGcZ"
   },
   "source": [
    "In a minute we will have to estimate probabilites of different texts. \n",
    "\n",
    "We could use *language models* using e.g. n-grams or recurrent neural networks, to calculate probability of original texts. \n",
    "\n",
    "But for our problem, it will be sufficient to represent each text with the set of words (and other symbols) that occur in it. This representation ignores word order and number of words.\n",
    "\n",
    "That is, we will not make difference between texts \n",
    "\n",
    "> this one is a long message. \n",
    "\n",
    "and \n",
    "\n",
    "> this message is a long long long long long long one.\n",
    "\n",
    "Both will be represented as a set of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0jl-tp0jSGcd",
    "outputId": "c57563cf-cbbb-40fd-8784-ec77bb7d259f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'caresses', 'is', 'long', 'message', 'one', 'ponies', 'this'}"
      ]
     },
     "execution_count": 219,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk import stem\n",
    "# from nltk.corpus import stopwords\n",
    "# stemmer = stem.SnowballStemmer('english')\n",
    "# stopwords = set(stopwords.words('english'))\n",
    "import nltk\n",
    "from nltk import stem\n",
    "#nltk.download()\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = stem.SnowballStemmer('english')\n",
    "\n",
    "#stopwords = stopwords.words(\"english\")\n",
    "#stopwords = [\"\", \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "#stopwords = [\"\", \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"couldn\", \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\", \"mightn't\", \"more\", \"most\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \"should've\", \"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\", \"weren't\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \"there's\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\", \"able\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afterwards\", \"ah\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"among\", \"amongst\", \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\", \"approximately\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\", \"asking\", \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"became\", \"become\", \"becomes\", \"becoming\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"believe\", \"beside\", \"besides\", \"beyond\", \"biol\", \"brief\", \"briefly\", \"c\", \"ca\", \"came\", \"cannot\", \"can't\", \"cause\", \"causes\", \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\", \"containing\", \"contains\", \"couldnt\", \"date\", \"different\", \"done\", \"downwards\", \"due\", \"e\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"except\", \"f\", \"far\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\", \"gotten\", \"h\", \"happens\", \"hardly\", \"hed\", \"hence\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hi\", \"hid\", \"hither\", \"home\", \"howbeit\", \"however\", \"hundred\", \"id\", \"ie\", \"im\", \"immediate\", \"immediately\", \"importance\", \"important\", \"inc\", \"indeed\", \"index\", \"information\", \"instead\", \"invention\", \"inward\", \"itd\", \"it'll\", \"j\", \"k\", \"keep\", \"keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\", \"looking\", \"looks\", \"ltd\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"million\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"nowhere\", \"obtain\", \"obtained\", \"obviously\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"one\", \"ones\", \"onto\", \"ord\", \"others\", \"otherwise\", \"outside\", \"overall\", \"owing\", \"p\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\", \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"put\", \"q\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\", \"rather\", \"rd\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\", \"several\", \"shall\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"thank\", \"thanks\", \"thanx\", \"thats\", \"that've\", \"thence\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"theyd\", \"theyre\", \"think\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"throughout\", \"thru\", \"thus\", \"til\", \"tip\", \"together\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"unto\", \"upon\", \"ups\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"'ve\", \"via\", \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"want\", \"wants\", \"wasnt\", \"way\", \"wed\", \"welcome\", \"went\", \"werent\", \"whatever\", \"what'll\", \"whats\", \"whence\", \"whenever\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"whim\", \"whither\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whomever\", \"whos\", \"whose\", \"widely\", \"willing\", \"wish\", \"within\", \"without\", \"wont\", \"words\", \"world\", \"wouldnt\", \"www\", \"x\", \"yes\", \"yet\", \"youd\", \"youre\", \"z\", \"zero\", \"a's\", \"ain't\", \"allow\", \"allows\", \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"associated\", \"best\", \"better\", \"c'mon\", \"c's\", \"cant\", \"changes\", \"clearly\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"corresponding\", \"course\", \"currently\", \"definitely\", \"described\", \"despite\", \"entirely\", \"exactly\", \"example\", \"going\", \"greetings\", \"hello\", \"help\", \"hopefully\", \"ignored\", \"inasmuch\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\", \"it'd\", \"keep\", \"keeps\", \"novel\", \"presumably\", \"reasonably\", \"second\", \"secondly\", \"sensible\", \"serious\", \"seriously\", \"sure\", \"t's\", \"third\", \"thorough\", \"thoroughly\", \"three\", \"well\", \"wonder\", \"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"co\", \"op\", \"research-articl\", \"pagecount\", \"cit\", \"ibid\", \"les\", \"le\", \"au\", \"que\", \"est\", \"pas\", \"vol\", \"el\", \"los\", \"pp\", \"u201d\", \"well-b\", \"http\", \"volumtype\", \"par\", \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"ac\", \"ad\", \"ae\", \"af\", \"ag\", \"aj\", \"al\", \"an\", \"ao\", \"ap\", \"ar\", \"av\", \"aw\", \"ax\", \"ay\", \"az\", \"b1\", \"b2\", \"b3\", \"ba\", \"bc\", \"bd\", \"be\", \"bi\", \"bj\", \"bk\", \"bl\", \"bn\", \"bp\", \"br\", \"bs\", \"bt\", \"bu\", \"bx\", \"c1\", \"c2\", \"c3\", \"cc\", \"cd\", \"ce\", \"cf\", \"cg\", \"ch\", \"ci\", \"cj\", \"cl\", \"cm\", \"cn\", \"cp\", \"cq\", \"cr\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d2\", \"da\", \"dc\", \"dd\", \"de\", \"df\", \"di\", \"dj\", \"dk\", \"dl\", \"do\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"dx\", \"dy\", \"e2\", \"e3\", \"ea\", \"ec\", \"ed\", \"ee\", \"ef\", \"ei\", \"ej\", \"el\", \"em\", \"en\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"et\", \"eu\", \"ev\", \"ex\", \"ey\", \"f2\", \"fa\", \"fc\", \"ff\", \"fi\", \"fj\", \"fl\", \"fn\", \"fo\", \"fr\", \"fs\", \"ft\", \"fu\", \"fy\", \"ga\", \"ge\", \"gi\", \"gj\", \"gl\", \"go\", \"gr\", \"gs\", \"gy\", \"h2\", \"h3\", \"hh\", \"hi\", \"hj\", \"ho\", \"hr\", \"hs\", \"hu\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ic\", \"ie\", \"ig\", \"ih\", \"ii\", \"ij\", \"il\", \"in\", \"io\", \"ip\", \"iq\", \"ir\", \"iv\", \"ix\", \"iy\", \"iz\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"ke\", \"kg\", \"kj\", \"km\", \"ko\", \"l2\", \"la\", \"lb\", \"lc\", \"lf\", \"lj\", \"ln\", \"lo\", \"lr\", \"ls\", \"lt\", \"m2\", \"ml\", \"mn\", \"mo\", \"ms\", \"mt\", \"mu\", \"n2\", \"nc\", \"nd\", \"ne\", \"ng\", \"ni\", \"nj\", \"nl\", \"nn\", \"nr\", \"ns\", \"nt\", \"ny\", \"oa\", \"ob\", \"oc\", \"od\", \"of\", \"og\", \"oi\", \"oj\", \"ol\", \"om\", \"on\", \"oo\", \"oq\", \"or\", \"os\", \"ot\", \"ou\", \"ow\", \"ox\", \"oz\", \"p1\", \"p2\", \"p3\", \"pc\", \"pd\", \"pe\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"pm\", \"pn\", \"po\", \"pq\", \"pr\", \"ps\", \"pt\", \"pu\", \"py\", \"qj\", \"qu\", \"r2\", \"ra\", \"rc\", \"rd\", \"rf\", \"rh\", \"ri\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"rv\", \"ry\", \"s2\", \"sa\", \"sc\", \"sd\", \"se\", \"sf\", \"si\", \"sj\", \"sl\", \"sm\", \"sn\", \"sp\", \"sq\", \"sr\", \"ss\", \"st\", \"sy\", \"sz\", \"t1\", \"t2\", \"t3\", \"tb\", \"tc\", \"td\", \"te\", \"tf\", \"th\", \"ti\", \"tj\", \"tl\", \"tm\", \"tn\", \"tp\", \"tq\", \"tr\", \"ts\", \"tt\", \"tv\", \"tx\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"uo\", \"ur\", \"ut\", \"va\", \"wa\", \"vd\", \"wi\", \"vj\", \"vo\", \"wo\", \"vq\", \"vt\", \"vu\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y2\", \"yj\", \"yl\", \"yr\", \"ys\", \"yt\", \"zi\", \"zz\"]\n",
    "#'win', 'luck', 'big', 'title'\n",
    "#\n",
    "#stopwords = [\"\", 'dhoni', \"so\", 'we', 'to',  'have', 'will', 'some']\n",
    "stopwords = [\"\"]\n",
    "def get_words(text):\n",
    "    \"\"\" This function converts the given text into an unordered and uncounted bag of words. \"\"\"\n",
    "    #return set(re.split('\\W+', text.lower())).difference({''} )\n",
    "    \n",
    "    return set(re.split('\\W+', text.lower())).difference(stopwords )\n",
    "    \n",
    "    #first_text = [word for word in re.split('\\W+', text.lower()) if word not in stopwords]\n",
    "    #stem_text = [stemmer.stem(word) for word in first_text]\n",
    "    #return set(stem_text)\n",
    "    #.difference(stopwords )\n",
    "\n",
    "# just an example\n",
    "get_words(\"this message is a long, long, long, long long long one caresses ponies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-3Gs7xsLSGch"
   },
   "source": [
    "This simplified approach will allow us to train the probabilistic model of texts using a modest amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vJbmpGfSSGci"
   },
   "outputs": [],
   "source": [
    "# apply this logic to texts of all messages\n",
    "bags_of_words = [get_words(text) for text in data.text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "woVdtF5PSGcm"
   },
   "source": [
    "To evaluate how well our model classifies messages, let's train it on the first 3000 texts, and measure accuracy on the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A2iDe2FLSGcn"
   },
   "outputs": [],
   "source": [
    "n_train = 3000\n",
    "train_x, test_x, train_y, test_y = bags_of_words[:n_train], bags_of_words[n_train:], data.target[:n_train], data.target[n_train:]\n",
    "# print(bags_of_words)\n",
    "# print(train_x)\n",
    "# print(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0pWrcoUWSGcu"
   },
   "source": [
    "## The basic classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2t_aDhsOSGcv"
   },
   "source": [
    "In the cell below, we will count occurences of words under different labels.\n",
    "\n",
    "We are going to use `Counter` objects. If you are not sure how they work, please look at [the documentation](https://docs.python.org/3.6/library/collections.html#collections.Counter). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "syP02W9jSGcw"
   },
   "outputs": [],
   "source": [
    "# this counter will keep the number of spam and ham texts\n",
    "label_counter = Counter()\n",
    "\n",
    "# these counters will keep the frequency of each word in ham and spam texts\n",
    "word_counters = {\n",
    "    'spam': Counter(), \n",
    "    'ham': Counter()\n",
    "}\n",
    "\n",
    "#print(\"nb spam:{}\".format(word_counters['spam']))\n",
    "all_words = set()\n",
    "\n",
    "for label, words in zip(train_y, train_x):\n",
    "    all_words.update(words)\n",
    "#    print(type(words))\n",
    "    # TODO: use the `update` methods of all 3 counters, to calculate total number of \n",
    "#    print(list(words))\n",
    "    label_counter.update({label:1})\n",
    "    word_counters[label].update(words)\n",
    "\n",
    "\n",
    "assert label_counter['spam'] == 409\n",
    "assert word_counters['ham']['hello'] >= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32trpvYkSGc0"
   },
   "source": [
    "Now let's calculate different probabilities of words, texts, and labels for our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-58zZ0MdSGc1"
   },
   "outputs": [],
   "source": [
    "def prior_probability_of_label(label):\n",
    "    \"\"\" This function evaluates probability of the given label (it can be 'spam' or 'ham'), using the counters. \"\"\"\n",
    "    # TODO: calculate and return this probability as ratio of number of texts with this labels to number all texts\n",
    "    return label_counter[label] / sum(label_counter.values())\n",
    "\n",
    "# print(label_counter['spam'])\n",
    "# print(len(list(label_counter)))\n",
    "# print(prior_probability_of_label('spam'))\n",
    "assert round(prior_probability_of_label('spam'), 2) == 0.14\n",
    "assert round(prior_probability_of_label('ham'), 2) == 0.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RB4bvSGjSGc5"
   },
   "outputs": [],
   "source": [
    "def word_probability_given_label(word, label):\n",
    "    \"\"\" This function calculates probability of a word occurence in text, conditional on the label of this text. \"\"\"\n",
    "    # TODO: calculate and return this probability \n",
    "    # as ratio of number of texts with this word and label to number of texts with this label\n",
    "\n",
    "    #return (word_counters[label][word] + 𝛼 * p) / (label_counter[label] + p)\n",
    "    \n",
    "    return (word_counters[label][word]) / (label_counter[label] )\n",
    "\n",
    "assert round(word_probability_given_label(\"99\", \"spam\"), 3) == 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0MQSlyxSGdA"
   },
   "source": [
    "Here we encounter the first practical problem: some words have never occurred in our training data. \n",
    "\n",
    "But they can probably occur in the texts to which our model will be applied in the future. \n",
    "\n",
    "To assign a non-zero probability to such texts, we can slightly modify the `word_probability_given_label`. For example, instead of original estimate, \n",
    "\n",
    "$$\\hat{p}(word|label) = \\frac{count(word, label)}{count(label)}$$\n",
    "\n",
    "we could use a \"smoothed\" version\n",
    "\n",
    "$$\\hat{p}(word|label) = \\frac{count(word, label) + \\alpha\\times p}{count(label) + p}$$\n",
    "\n",
    "where $alpha\\in(0, 1)$ is the anchor probability towards which we move our estimate, and $p$ is the step size towards this anchor. \n",
    "\n",
    "Values like $p=0.1$ and $\\alpha=1^{-3}$ would do.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63mpxYPlSGdC"
   },
   "outputs": [],
   "source": [
    "# TODO: modify the `word_probability_given_label` function, by moving each probability towards a small positive constant\n",
    "def word_probability_given_label(word, label):\n",
    "    \"\"\" This function calculates probability of a word occurence in text, conditional on the label of this text. \"\"\"\n",
    "    # TODO: calculate and return this probability \n",
    "    # as ratio of number of texts with this word and label to number of texts with this label\n",
    "    p = 0.009\n",
    "    𝛼 = 1 ** -3\n",
    "\n",
    "    return (word_counters[label][word] + 𝛼 * p) / (label_counter[label] + p)\n",
    "    #return (word_counters[label][word]) / (label_counter[label] )\n",
    "\n",
    "assert word_probability_given_label(\"999\", \"spam\") > 0\n",
    "assert word_probability_given_label(\"999\", \"spam\") < 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-LeclJfNSGdG"
   },
   "source": [
    "Now we can move from words to texts. \n",
    "\n",
    "Here is where we apply our naive assumption that occurrences of each word are independent:\n",
    "$$ P(text|label) = \\prod_{word \\in text} P(word|label) \\times \\prod_{word \\notin text} (1-P(word|label)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iVZdBCwYSGdH"
   },
   "outputs": [],
   "source": [
    "def text_probability_given_label(text, label):\n",
    "    \"\"\" This function calculates probability of the text conditional on its label. \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = get_words(text)\n",
    "    probability = 1.0\n",
    "    # TODO: calculate the probability of text given label. \n",
    "    # use a function defined above and the naive assumption of word independence\n",
    "    for word in all_words:\n",
    "        if word in text:\n",
    "            probability *= word_probability_given_label(word, label) \n",
    "            #pass \n",
    "        else:\n",
    "          probability *=  (1 - word_probability_given_label(word, label))\n",
    "            #pass\n",
    "    return probability\n",
    "\n",
    "greeting1 = 'hello how are you'\n",
    "greeting2 = 'hello teacher how are you'\n",
    "\n",
    "assert text_probability_given_label(greeting1, 'ham') > 0\n",
    "assert text_probability_given_label(greeting1, 'ham') < 0.0001\n",
    "assert text_probability_given_label(greeting2, 'ham') < text_probability_given_label(greeting1, 'ham')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XQgZXCuDSGdM"
   },
   "source": [
    "Now you have all the components to compile your first probabilistic classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "6OrHsTALSGdO",
    "outputId": "cbe43cf8-2eb6-40d3-8afe-8ff62da889b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999999459718435\n",
      "5.40281565e-07\n",
      "new\n",
      "0.112191797747459\n",
      "0.887808202252541\n",
      "a:1.0\n"
     ]
    }
   ],
   "source": [
    "def label_probability_given_text(text, label):\n",
    "    \"\"\" This function calculates probability of the label (spam or ham) conditional on the text. \"\"\"\n",
    "    # TODO: calculate label probability conditional on text\n",
    "    # use the Bayes rule and the functions defined above\n",
    "\n",
    "  #round\n",
    "    return round((prior_probability_of_label(label) * text_probability_given_label(text, label)) / \n",
    "     (text_probability_given_label(text, \"spam\") * prior_probability_of_label(\"spam\") + text_probability_given_label(text, \"ham\") * prior_probability_of_label(\"ham\")),15)\n",
    "#𝑃(𝑠𝑝𝑎𝑚|𝑡𝑒𝑥𝑡)=𝑃(𝑠𝑝𝑎𝑚)𝑃(𝑡𝑒𝑥𝑡|𝑠𝑝𝑎𝑚)𝑃(𝑡𝑒𝑥𝑡)\n",
    "\n",
    "\n",
    "text1 = 'hello how r you'\n",
    "text2 = 'only today you can buy our book with 50% discount!'\n",
    "\n",
    "print(label_probability_given_text(text1, 'ham'))\n",
    "print(label_probability_given_text(text1, 'spam'))\n",
    "#a = word_probability_given_label(text1, \"\") \n",
    "#print(\"all_words:{}\".format(len(all_words)))\n",
    "print(\"new\")\n",
    "print(label_probability_given_text(text2, 'ham'))\n",
    "print(label_probability_given_text(text2, 'spam'))\n",
    "a = label_probability_given_text(text1, 'ham') + label_probability_given_text(text1, 'spam')\n",
    "print(\"a:{}\".format(a))\n",
    "assert label_probability_given_text(text1, 'ham') + label_probability_given_text(text1, 'spam') == 1.0\n",
    "assert label_probability_given_text(text1, 'ham') > label_probability_given_text(text1, 'spam')\n",
    "assert label_probability_given_text(text1, 'ham') > label_probability_given_text(text2, 'ham')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "25DmEePrSGdR"
   },
   "source": [
    "## Tuning the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SVNJNZimSGdS"
   },
   "source": [
    "Now we have the classifier, but we don't know how well it works on the unseen data. \n",
    "\n",
    "Let's see what fraction of test messages are classified correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1ZxEYeFGSGdT",
    "outputId": "0d40d1bb-5a54-48d9-ae25-568423d292d1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-46c72122c3aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#threshold = 0.33\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.004\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_spam_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel_probability_given_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'spam'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'spam'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mspamness\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'ham'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mspamness\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_spam_probabilities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_x' is not defined"
     ]
    }
   ],
   "source": [
    "#threshold = 0.33\n",
    "threshold = 0.004\n",
    "test_spam_probabilities = [label_probability_given_text(text, 'spam') for text in test_x]\n",
    "test_predictions = ['spam' if spamness > threshold else 'ham' for spamness in test_spam_probabilities]\n",
    "\n",
    "\n",
    "\n",
    "accuracy = sum(1 if pred == fact else 0 for pred, fact in zip(test_predictions, test_y)) / len(test_y)\n",
    "print(accuracy)\n",
    "\n",
    "# i = 0\n",
    "# cp_spam = 0\n",
    "# cp_ham = 0\n",
    "# for pred, fact in zip(test_predictions, test_y):\n",
    "#   if pred != fact:\n",
    "#     if fact == \"spam\":\n",
    "#       cp_spam += 1\n",
    "#     else:\n",
    "#       cp_ham += 1\n",
    "#     print (fact)\n",
    "#     print(test_spam_probabilities[i])\n",
    "#     print(test_x[i])\n",
    "#   i += 1\n",
    "# print(cp_spam) \n",
    "# print(cp_ham)\n",
    "\n",
    "assert accuracy > 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qorvR9XvSGdo"
   },
   "source": [
    "This is a good accuracy, but you can achieve better results by tuning the algorithm. \n",
    "\n",
    "What you can do:\n",
    "* play with the different values of the threshold\n",
    "* play with the regularization constants that you used in `word_probability_given_label`\n",
    "* experiment with different implementations of `get_words` - e.g. ignore the word case, or use word lemmas\n",
    "* use your imagination\n",
    "\n",
    "Can you beat 99% accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sGSStXnGSGd3"
   },
   "source": [
    "Have a good time! (-:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y0eQuYMyZM7f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Samuel Guedj - naive_bayes_classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
